{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fcc553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import huggingface_hub\n",
    "\n",
    "HF_TOKEN = \"your_huggingface_token_here\"\n",
    "huggingface_hub.login(token=HF_TOKEN)\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "\n",
    ")\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "model_id = \"Qwen/Qwen3-30B-A3B-Instruct-2507-FP8\"\n",
    "model = LLM(model=model_id, max_model_len=32768)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    padding_side=\"left\",\n",
    "    truncation_side=\"left\",\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "samp_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    top_k=20,\n",
    "    min_p=0.0,\n",
    "    max_tokens=16384,\n",
    ")\n",
    "dataset_lar = load_dataset(\"Yeongtak/lar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b956449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_judge_prompt(ground_truth, response):\n",
    "    prompt = f\"\"\"You are an impartial judge evaluating whether a model's response correctly matches a ground-truth reference.\n",
    "\n",
    "You will be given:\n",
    "1. A question asked to the model\n",
    "2. A ground-truth reference answer (GT)\n",
    "3. A generated response from the model\n",
    "\n",
    "Your task is to decide whether the generated response is Correct or Wrong.\n",
    "\n",
    "Evaluation criteria:\n",
    "- The generated response is Correct if it semantically includes the core information conveyed by the ground-truth reference.\n",
    "- The wording does NOT need to match exactly. Paraphrases, rephrasings, or additional details are allowed.\n",
    "- The generated response may contain extra information beyond the ground-truth reference. This is acceptable.\n",
    "- The generated response is Wrong if it:\n",
    "  - Fails to include the core meaning of the ground-truth reference, OR\n",
    "  - Contradicts the ground-truth reference, OR\n",
    "  - Provides an unrelated or vague answer that does not clearly convey the same experience or action.\n",
    "\n",
    "Focus ONLY on whether the essential meaning of the ground-truth reference is present in the generated response.\n",
    "\n",
    "Do NOT judge based on style, fluency, length, or factual completeness beyond the ground-truth reference.\n",
    "\n",
    "Output:\n",
    "- Respond with exactly one word: Correct or Wrong.\n",
    "\n",
    "---\n",
    "\n",
    "Question:\n",
    "What was I doing the last time I told you about my most recent experience with the one in the new image?\n",
    "\n",
    "Ground-truth reference:\n",
    "User said \"{ground_truth}\"\n",
    "\n",
    "Generated response:\n",
    "{response}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a30def",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "with open(\"path_to_your_caption.json\") as f:\n",
    "    responses = json.load(f)\n",
    "    \n",
    "evals = []\n",
    "for i in tqdm(range(0, len(dataset_lar), batch_size)):\n",
    "    bs = min(batch_size, len(dataset_lar) - i)\n",
    "    prompts = []\n",
    "    for j in range(i, i + bs):\n",
    "        judge_prompt = get_judge_prompt(dataset_lar[j][\"last_action\"], responses[j][\"response\"])\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": judge_prompt},\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    outputs = model.generate(prompts, sampling_params=samp_params, use_tqdm=False)\n",
    "    for j in range(bs):\n",
    "        evals.append(outputs[j].outputs[0].text.strip())\n",
    "\n",
    "corrects = []\n",
    "for eval in evals:\n",
    "    if \"correct\" in eval.lower():\n",
    "        corrects.append(1)\n",
    "    else:\n",
    "        corrects.append(0)\n",
    "accuracy = sum(corrects) / len(corrects)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmmlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
