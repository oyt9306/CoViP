{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0eabb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_captioning = \"\"\"You are an AI model that can perceive multiple past dialogues and use them as memory to personalize your description of a new image.\n",
    "\n",
    "[Context]\n",
    "You have been given several past dialogues.\n",
    "Each dialogue contains an image and a corresponding conversation between a user and you.\n",
    "These conversations describe specific objects (people, animals, items, or places) along with contextual details such as names, locations, times, and experiences.\n",
    "This entire context represents your prior shared experiences with the user.\n",
    "\n",
    "[Task]\n",
    "Now, you are given a **new image** that may include one or more of the same objects mentioned in the previous dialogues.\n",
    "Your goal is to describe this new image **by integrating relevant information from the context**.\n",
    "\n",
    "Follow these rules carefully:\n",
    "\n",
    "1. **Recall and reuse details** from the previous dialogues (object names, appearances, places, times, and relationships).\n",
    "   - Treat the previous dialogues as your long-term memory.\n",
    "   - If an object in the new image appears similar to one mentioned in the past, refer to it using the same name and contextual background.\n",
    "\n",
    "2. **Ground your description in the new image’s visual content.**\n",
    "   - Accurately describe what you see: composition, setting, lighting, and object state.\n",
    "   - Then integrate remembered details from the context naturally (e.g., “This looks like Pino again, perhaps older than in the park photo from Busan Station.”).\n",
    "\n",
    "3. Keep your tone natural and human-like — as if you’re describing something familiar to the same user.\n",
    "\n",
    "4. Do not restate previous dialogues verbatim. Instead, synthesize and extend them with new image-grounded observations.\n",
    "\n",
    "5. Write in paragraph form, not in a dialogue format.\n",
    "\n",
    "6. **Use only relevant memories.**\n",
    "   - If an object or scene described in the previous dialogues does **not appear in the new image**, ignore it completely.\n",
    "   - Include contextual information **only for the objects that actually appear** in the new image.\n",
    "   - Avoid bringing up unrelated names, locations, or events from the past context.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede4ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    ")\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\" # Use GPU 0\n",
    "\n",
    "def prepare_inputs_for_vllm(messages, processor):\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True, add_vision_id=True\n",
    "    )\n",
    "    # qwen_vl_utils 0.0.14+ reqired\n",
    "    image_inputs, video_inputs, video_kwargs = process_vision_info(\n",
    "        messages,\n",
    "        image_patch_size=processor.image_processor.patch_size,\n",
    "        return_video_kwargs=True,\n",
    "        return_video_metadata=True,\n",
    "    )\n",
    "    # print(f\"video_kwargs: {video_kwargs}\")\n",
    "\n",
    "    mm_data = {}\n",
    "    if image_inputs is not None:\n",
    "        mm_data[\"image\"] = image_inputs\n",
    "    if video_inputs is not None:\n",
    "        mm_data[\"video\"] = video_inputs\n",
    "\n",
    "    return {\n",
    "        \"prompt\": text,\n",
    "        \"multi_modal_data\": mm_data,\n",
    "        \"mm_processor_kwargs\": video_kwargs,\n",
    "    }\n",
    "\n",
    "\n",
    "model_size = '8B' # '30B'\n",
    "model_id = 'Yeongtak/CoViP-Qwen3-VL-8B-GSPO' \n",
    "model = LLM(model=model_id, max_model_len=8192, gpu_memory_utilization=0.8) # hard setting for A40 46Gb VRAM\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0,\n",
    "    max_tokens=1024,\n",
    "    top_k=-1,\n",
    "    stop_token_ids=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2d8ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import re\n",
    "import copy \n",
    "import ast\n",
    "import random \n",
    "from tqdm import tqdm \n",
    "from pathlib import Path\n",
    "\n",
    "random.seed(42)\n",
    "name = 'CoViP_testset'\n",
    "jsonl_path = Path(f\"{name}/meta_{name}_cleaned.jsonl\") \n",
    "prefix = f'path_to_your_data'\n",
    "mode = 'CoViP-Qwen3-VL-8B-GSPO' \n",
    "total = []\n",
    "\n",
    "with jsonl_path.open(\"r\", encoding=\"utf-8\") as src:\n",
    "    for lineno, line in enumerate(src, start=1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        record = json.loads(line)\n",
    "        total.append(record)\n",
    "        \n",
    "def return_encoded_query(example):\n",
    "    img_path = prefix + example[\"imgs\"]  \n",
    "    return img_path\n",
    "\n",
    "def return_encoded_concepts(example):\n",
    "    imgs, texts = [], [] \n",
    "    dialogues = example['dialogues']\n",
    "    diags = []\n",
    "    for index, (key, value) in enumerate(dialogues.items()):\n",
    "        diags.append(value)\n",
    "    \n",
    "    cnt = 0\n",
    "    pairs = []\n",
    "    for img in example['concepts']:        \n",
    "        img_path = prefix + img   \n",
    "        pairs.append((diags[cnt], img_path))\n",
    "        cnt += 1\n",
    "    return pairs\n",
    "\n",
    "def build_multimodal_content(context_pairs, query_text=None, query_image=None):\n",
    "    content = []\n",
    "\n",
    "    j = 0\n",
    "    for text, img in context_pairs:\n",
    "        content.append({\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"===== Dialogue {j} =====\\n\",\n",
    "            })\n",
    "        if img is not None:\n",
    "            content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": img\n",
    "            })\n",
    "        if text is not None:\n",
    "            content.append({\n",
    "                \"type\": \"text\",\n",
    "                \"text\": text,\n",
    "            })\n",
    "        j += 1\n",
    "        \n",
    "    content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"===== New Image =====\\n\",\n",
    "        })\n",
    "    if query_image is not None:\n",
    "        content.append({\n",
    "            \"type\": \"image\",\n",
    "            \"image\": query_image,\n",
    "        })\n",
    "    if query_text is not None:\n",
    "        content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": query_text,\n",
    "        })\n",
    "    return content\n",
    "\n",
    "\n",
    "def return_response_batch(mode, examples):\n",
    "    inputs_list = []\n",
    "\n",
    "    for example in examples:\n",
    "        context_pairs = return_encoded_concepts(example)\n",
    "        query_text  = prompt_captioning\n",
    "        base64_qwen = return_encoded_query(example)\n",
    "        query_image = base64_qwen  \n",
    "\n",
    "        content = build_multimodal_content(\n",
    "            context_pairs=context_pairs,\n",
    "            query_text=query_text,\n",
    "            query_image=query_image,\n",
    "        )\n",
    "        message = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content,\n",
    "            },\n",
    "        ]\n",
    "        inputs = prepare_inputs_for_vllm(message, processor)\n",
    "        inputs_list.append(inputs)\n",
    "\n",
    "    outputs = model.generate(inputs_list, sampling_params=sampling_params, use_tqdm=False)\n",
    "    return outputs  # len(outputs) == len(examples)\n",
    "\n",
    "\n",
    "tot_ranges = list(range(len(total)))\n",
    "caption = {}\n",
    "\n",
    "for idx in tot_ranges:\n",
    "    caption[f'sample_{idx}'] = {}\n",
    "    caption[f'sample_{idx}']['caption'] = []\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "for i in tqdm(range(0,len(tot_ranges), batch_size)):\n",
    "    try:\n",
    "        batch_indices = tot_ranges[i : i + batch_size]           # 예: [0,1,2,3]\n",
    "        batch_examples = [total[idx] for idx in batch_indices]   # 해당 example들\n",
    "        batch_outputs = return_response_batch(mode, batch_examples)\n",
    "        for idx, output in zip(batch_indices, batch_outputs):\n",
    "            caption[f'sample_{idx}']['caption'] = [output]\n",
    "    except:\n",
    "        pass\n",
    "lists = list(caption.keys())\n",
    "\n",
    "caption_renew = {}\n",
    "for name in lists:\n",
    "    caption_renew[name] = {}\n",
    "    caption_renew[name]['caption'] = []\n",
    "    \n",
    "    \n",
    "for key in lists:\n",
    "    try:\n",
    "        text = caption[key]['caption'][0].outputs[0].text\n",
    "        caption_renew[key] = text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "save_path = f\"captions_{mode}_test.json\"\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(caption_renew, f,  indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d749b81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmmlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
